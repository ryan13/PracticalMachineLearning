---
title: "finalProject"
author: "tri doan"
date: "Sunday, April 26, 2015"
output: html_document
---

In this project, a large collection of data  from Jawbone Up, Nike FuelBand, and Fitbit of data about personal activity. These information can reveal the patterns in users' behavior. The tast is to quantify how much of a particular activity users. Data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. For its simplicity, I download  two datasets and do exploratory analysis

At first, I see blank columns that in fact contains kind of data like "#DIV/0!" which I replace as NA when data is read. All missing data has no use in prediction, I check to see any column with all missing data which is also a subject to remove.
The following code is refered from stackoverflow


```{r}
df <- read.csv("pml-training.csv", na.strings=c("#DIV/0!") )


#To find the columns with all values missing

allmisscols <- apply(df,2, function(x)all(is.na(x)));  
colswithallmiss <-names(allmisscols[allmisscols>0]);    
print("the columns with all values missing");    
print(colswithallmiss);

```
 There are 6 columns of all missing values which will be eleminated in the next followng steps. Beside that, first column indicates the order of data, and 6 following columns have no use for the purpose of this project also are all eliminated. Further, we convert features into numeric except classe 
```{r}
classe <- df$classe
df<- as.data.frame(lapply(df[-ncol(df)],as.numeric))
df$classe <- classe
features <- colnames(df[colSums(is.na(df)) == 0])[-c(1:7)]
df <- df[features]
str(df)

```
Now we have data frame with 120 columns and 19622 data examples. I implement random forest this project but have no time to see any other methods. Now we load libraries for data mining
```{r}

library(caret)
library(randomForest)
library(foreach)
library(doParallel)
```
First step to create training and testing data using caret package
```{r}
TrainIndex <- createDataPartition(y=df$classe, p=0.70, list=FALSE )
trainDat <- df[TrainIndex,]
testDat <- df[-TrainIndex,]

```
We try random forests which can make use parallel processing to reduce run time using doParallel package. we can build 5 random forest with 150 trees each.
```{r}
registerDoParallel()
trainx <- trainDat[-ncol(trainDat)]
trainy <- trainDat$classe

rf <- foreach(ntree=rep(150, 5), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(trainx, trainy, ntree=ntree) 
}
```

In sample error rate:
```{r}
prediction <- predict(rf, newdata=trainDat)
confusionMatrix(prediction,trainDat$classe)[3]
```
We see that high accuracy which in-sample error is 0
We look at out of sample error
```{r}
prediction <- predict(rf, newdata=testDat)
confusionMatrix(prediction,testDat$classe)
```
This shows that the model gives out high accuracy of 0.9939 or 99.39%. In fact, by calculating the out of sample error (the cross-validation estimate is an out-of-sample estimate), we see that the sample error rate is about 0.0061 or 0.61%  

Using above model, we have the predictions for the test dataset:
```{r}

df2 <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!") )

df2<- as.data.frame(lapply(df2[-ncol(df2)],as.numeric))


df2 <- df2[features[features!='classe']]
predictTest <- predict(rf, newdata=df2)
predictTest

```
We can output the prediction in files
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictTest)
