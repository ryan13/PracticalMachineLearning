submit()
info()
play()
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit <- lm(y~x)
summary(fit)
data(mtcars)
fit <- lm(mpg~weight)
fit <- lm(mpg~weight,data=mtcars)
>lm
?lm
str(mtcars)
fit <- lm(mtcars$mpg~mtcars$weight,data=mtcars)
mtcars$weigth
mtcars$weight
mtcars$mpg
mtcars$wt
fit <- lm(mtcars$mpg~mtcars$wt,data=mtcars)
summary(fit)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
mean(x)
sd(x)
(8.58 - 9.31)/sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit <- lm(y~x)
summary(fit)
x <- c(0.18, -1.54, 0.42, 0.95)
mean(x)
quit
90
quit()
library(swirl)
swirl()
submit()
extract_numeric("class5")
extract_numeric("2class5")
extract_numeric("2cla5ss5")
t <- c("class1","class2","class3","class4")
extract_numeric(t)
dat <- as.data.frame(t)
class(dat)
extract_numeric(dat)
dat
extract_numeric(dat$t)
submit()
submit()
submit()
submit()
submit()
extract_numeric(students3$class)
as.list(extract_numeric(students3$class))
students3$class
str(stunts3)
str(students3)
submit()
submit()
?install_course_github
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
students3 %>%
gather(class, grade, class1:class5, na.rm = TRUE) %>%
spread(test, grade)
students3 %>%
gather(class, grade, class1:class5, na.rm = TRUE) %>%
spread(test, grade) %>%  mutate(extract_numeric(class) )
### Call to mutate() goes here %>%
print
submit()
submit()
submit()
submit()
info()
skip()
View(student4)
students4
submit()
submit()
submit()
submit()
skip()
submit()
submit()
passed
failed
?mutate
mutate(passed,status="passed")
passed <- passed %>% mutate(status = "passed")
failed <- failed %>% mutate(status = "failed")
passed, failed  %>% bind_rows
passed failed  %>% bind_rows
bind_rows(passed,failed)
sat
submit()
submit()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package = lubridate)
today()
this_day <- today()
this_day
year(this_day)
wday(this_day)
wday(this_day,label=TRUE)
this_moment <- now()
this_moment
second(this_moment)
ymd("1989-05-17")
my_date <- ymd("1989-05-17")
my_date
class(my)_date
class(my_date)
ymd("1989 May 17")
mdy("March 12, 1975")
mdy(25081985)
dmy(25081985)
ymd("192012")
ymd("1920/1/2")
dt1
ymd_hms(dt1)
hms("03:22:14")
dt2
ymd(dt2)
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
this_moment <- update(this_moment, hours = 8, minutes = 34 )
this_moment
skip()
skip()
depart <-  nyc + days(2)
depart
depart <- update(depart,hours=17,minutes=34)
depart
arrive <- update(depart,hours=15,minutes=50)
arrive <- depart + hours(15) + minutes(50)
?with_tz
with_tz( "Asia/Hong_Kong")
arrive <- with_tz( "Asia/Hong_Kong")
arrive <- with_tz( arrive,"Asia/Hong_Kong")
arrive
last_time <- mdy("June 17, 2008",tz = "Singapore")
last_time
new_interval(arrive,last_time)
?new_interval
how_long <- new_interval(last_time,arrive,tzone=attr("Singapore"))
skip()
how_long <- new_interval(last_time, arrive)
as.period(how_long)
stopwatch()
swirl()
install_from_swirl("Mathematical Biostatistics Boot Camp")
install_from_swirl("R Programming")
install_from_swirl(" Data Analysis")
install_from_swirl("Statistical Inference")
install.packages("neuralnet")
library(nnet)
library(swirl)
swirl()
5 + 7
x <- 5 + 7
x
y <- x-3
y
c(1.1, 9, 3.14)
z <- c(1.1, 9, 3.14)
?c
z
c(z,555,z)
z * 2 + 100
my_sqrt <- swrt(z-1)
my_sqrt <- sqrt(z-1)
my_sqrt
my_div z/my_sqrt
my_div <- z/my_sqrt
my+div
my_div
c(1, 2, 3, 4) + c(0, 10)
c(1, 2, 3, 4) + c(0, 10, 100)
c(1, 2, 3, 4) + c(0, 10, 100)
c(1, 2, 3, 4) + c(0, 10)
z * 2 + 1000
my_sqrt
my_div
x
x[1:10]
x[is.na(x)]
y <- x[!is.na(x)]
y
y[y > 0]
x[x > 0]
x[!is.na(x) & x > 0]
x[c(3,5,7)]
x[0]
x[3000]
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)$sigma
summary(fit)
data(mtcars)
str(mtcars)
fit <- lm(mpg~wt,data=mtcars)
sumCoef <- summary(fit)$coefficients
sumCoef[2,1] + c(-1,1)*qt(0.975,df=fit$df) * sumCoef[2,2]
sumCoef[1,1] + c(-1,1)*qt(0.975,df=fit$df) * sumCoef[1,2]
help(mtcars)
library(Hmisc)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
colnames(concrete)
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
index <- seq_along(1:nrow(training))
index
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
From this plot we should probably cut the outcome in 4 categories
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
featurePlot(x = training[, names], y = cutCS, plot = "box")
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.8)
preProc$rotation
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
A1
modelFit <- train(training$diagnosis ~ ., method = "glm", preProcess = "pca",
data = training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
C2
A2 <- C2$overall[1]
A2
install.packages("IPSUR", dependencies = TRUE)
library(IPSUR)
read(IPSUR)
quit()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
index <- seq_along(1:nrow(training))
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
library(Hmisc)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +  theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +  theme_bw()
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain, ]
testing = mixtures[-inTrain, ]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
fit2$coefficients[3]
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl), data = mtcars)
fit2$coefficients
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt + interaction(cyl, wt), data = mtcars)
compare <- anova(fit1, fit2)
compare$Pr
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the hat diagonal for the most influential point
fit <- lm(y ~ x)
hatvalues(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
# Give the slope dfbeta for the point with the highest hat value.
fit <- lm(y ~ x)
dfbetas(fit)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
library(rattle)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
set.seed(125)
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
library(randomForest)
install.packages("pgmm")
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
library(randomForest)
model <- train(Area ~ ., data = olive, method = "rpart2")
newdata = as.data.frame(t(colMeans(olive)))
predict(model, newdata = newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
model <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, predict(model, newdata = testSA))
missClass(trainSA$chd, predict(model, newdata = trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(rattle)
summary(segmentationOriginal$Case)
inTrain <- grep("Train",segmentationOriginal$Case)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
fit <- train(Class~.,data=training,method="rpart")
fancyRpartPlot(fit$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
#TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2
#FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(fit,predData)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
## Loading required package: lattice
## Loading required package: ggplot2
library(rpart)
library(ggplot2)
library(rattle)
training<-segmentationOriginal[segmentationOriginal$Case=="Train",]
testing<-segmentationOriginal[segmentationOriginal$Case=="Test",]
set.seed(125)
model<-train(Class ~ .,data = training, method = "rpart")
summary(segmentationOriginal$Case)
inTrain <- grep("Train",segmentationOriginal$Case)
inTrain
inTrain <- grep("Train",segmentationOriginal$Case)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
fit <- train(Class~.,data=training,method="rpart")
fancyRpartPlot(fit$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
#TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2
#FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(fit,predData)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
Library(caret)
library(caret)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(randomForest)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
imps
library(randomForest)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ as.factor(cyl) + wt, data=mtcars)
summary(fit) # as.factor(cyl)8  -6.0709
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr # 0.1037502
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data=mtcars)
summary(fit4)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
lm.influence(fit5)$hat[5] # 0.9945734
hatvalues(fit5)
dfbetas(fit5)[5, 2] # -133.8226
library(dplyr)
install.packages("hflights")
glimpse(hflights)
library(hflights)
glimpse(hflights)
setwd("C:/practicalMachineLearning")
?rfcv
library(caret)
?rfcv
library(randomForest)
?rfcv
